{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dcaf893-07a3-4ac5-bc74-a0e580cfb8f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 02_bronze_copy_into_zip_time_series\n",
    "## Loads Zip_time_series parts into Bronze using COPY INTO (incremental simulation).\n",
    "## Audit cols are added during insert into final bronze table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b6a303c-999e-410c-885b-c4806ccfb5ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "CAT = \"zillow\"\n",
    "BRONZE = \"zillow_bronze\"\n",
    "\n",
    "BASE = \"/Volumes/zillow/zillow_medallion/raw-converted_format/incremental/zip_time_series\"\n",
    "ZIP_PARTS = [\n",
    "  f\"{BASE}/zip_part_1.csv\",\n",
    "  f\"{BASE}/zip_part_2.csv\",\n",
    "  f\"{BASE}/zip_part_3.csv\",\n",
    "  f\"{BASE}/zip_part_4.csv\",\n",
    "]\n",
    "\n",
    "stg = f\"{CAT}.{BRONZE}.zip_ts_stg\"\n",
    "tgt = f\"{CAT}.{BRONZE}.zip_ts_bronze\"\n",
    "\n",
    "# Checkpoint / helper location (optional)\n",
    "print(\"Parts:\", ZIP_PARTS)\n",
    "\n",
    "# COMMAND ----------\n",
    "# 1) Create staging table (schema inferred once from part_1)\n",
    "df0 = (spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(ZIP_PARTS[0]))\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {stg}\")\n",
    "df0.limit(0).write.format(\"delta\").mode(\"overwrite\").saveAsTable(stg)\n",
    "\n",
    "# 2) Create target bronze table with audit cols\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {tgt}\n",
    "USING DELTA\n",
    "AS SELECT\n",
    "  s.*,\n",
    "  CAST(NULL AS TIMESTAMP) AS load_dt,\n",
    "  CAST(NULL AS STRING) AS source_path,\n",
    "  CAST(NULL AS STRING) AS ingest_mode,\n",
    "  CAST(NULL AS INT) AS part_id\n",
    "FROM {stg} s\n",
    "WHERE 1=0\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"COMMENT ON TABLE {tgt} IS 'Zip time series Bronze. Loaded via COPY INTO per part arrival; audit cols added at insert.'\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# 3) Load each part (COPY INTO -> STG, then INSERT into TGT with audit cols)\n",
    "for i, part_path in enumerate(ZIP_PARTS, start=1):\n",
    "    print(f\"\\n--- Loading part {i}: {part_path}\")\n",
    "\n",
    "    # Clear staging\n",
    "    spark.sql(f\"TRUNCATE TABLE {stg}\")\n",
    "\n",
    "    # COPY INTO staging (requirement)\n",
    "    spark.sql(f\"\"\"\n",
    "    COPY INTO {stg}\n",
    "    FROM '{part_path}'\n",
    "    FILEFORMAT = CSV\n",
    "    FORMAT_OPTIONS ('header'='true', 'inferSchema'='true')\n",
    "    \"\"\")\n",
    "\n",
    "    # Insert into final with audit cols\n",
    "    spark.sql(f\"\"\"\n",
    "    INSERT INTO {tgt}\n",
    "    SELECT\n",
    "      s.*,\n",
    "      current_timestamp() AS load_dt,\n",
    "      '{part_path}' AS source_path,\n",
    "      'copy_into_csv' AS ingest_mode,\n",
    "      {i} AS part_id\n",
    "    FROM {stg} s\n",
    "    \"\"\")\n",
    "\n",
    "    rows = spark.sql(f\"SELECT COUNT(*) AS c FROM {stg}\").collect()[0][\"c\"]\n",
    "    print(\"Rows inserted:\", rows)\n",
    "\n",
    "# Quick evidence\n",
    "display(spark.sql(f\"SELECT part_id, COUNT(*) cnt FROM {tgt} GROUP BY part_id ORDER BY part_id\"))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6731695275405735,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_bronze_copy_into_zip_time_series",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
