{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31b3f26b-ab73-471f-9998-0b5df6d68609",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 05_bronze_ingest_supporting_files\n",
    "## One-time ingestion of supporting CSV + metadata JSON into Bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54da9c9f-7c11-40cf-8fe4-5a82124c0d82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "CAT = \"zillow\"\n",
    "BRONZE = \"zillow_bronze\"\n",
    "\n",
    "ASIS = \"/Volumes/zillow/zillow_medallion/raw-converted_format/as_is\"\n",
    "\n",
    "# CSV files to land as Bronze reference tables\n",
    "CSV_FILES = {\n",
    "  \"City_time_series.csv\": \"city_ts_bronze\",\n",
    "  \"Metro_time_series.csv\": \"metro_ts_bronze\",\n",
    "  \"CountyCrossWalk_Zillow.csv\": \"county_crosswalk_bronze\",\n",
    "  \"cities_crosswalk.csv\": \"cities_crosswalk_bronze\",\n",
    "  \"DataDictionary.csv\": \"data_dictionary_bronze\",\n",
    "}\n",
    "\n",
    "# JSON metadata files\n",
    "ALL_METRICS_JSON = f\"{ASIS}/all_available_metrics.json\"\n",
    "FIELDS_PER_LEVEL_JSON = f\"{ASIS}/fields_per_level.json\"\n",
    "\n",
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CAT}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CAT}.{BRONZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18ef7edf-ae23-4d93-b8f7-47f2ddbf0450",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Helper: create a staging table schema from the file (fast and simple)\n",
    "def create_stg_from_csv(path: str, stg_table: str):\n",
    "    df = (spark.read.option(\"header\",\"true\").option(\"inferSchema\",\"true\").csv(path))\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {stg_table}\")\n",
    "    df.limit(0).write.format(\"delta\").mode(\"overwrite\").saveAsTable(stg_table)\n",
    "    return df.schema\n",
    "\n",
    "def ensure_target_from_stg(stg: str, tgt: str, comment: str):\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {tgt}\n",
    "    USING DELTA\n",
    "    AS SELECT\n",
    "      s.*,\n",
    "      CAST(NULL AS TIMESTAMP) AS load_dt,\n",
    "      CAST(NULL AS STRING) AS source_path,\n",
    "      CAST(NULL AS STRING) AS ingest_mode\n",
    "    FROM {stg} s\n",
    "    WHERE 1=0\n",
    "    \"\"\")\n",
    "    spark.sql(f\"COMMENT ON TABLE {tgt} IS '{comment}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab1f7366-b488-4fe0-9226-751da42ca3d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1) CSVs via COPY INTO (one-time loads)\n",
    "for fname, tname in CSV_FILES.items():\n",
    "    src = f\"{ASIS}/{fname}\"\n",
    "    stg = f\"{CAT}.{BRONZE}._stg_{tname}\"\n",
    "    tgt = f\"{CAT}.{BRONZE}.{tname}\"\n",
    "\n",
    "    print(f\"\\n=== CSV -> COPY INTO: {fname} ===\")\n",
    "\n",
    "    # create staging with inferred schema\n",
    "    _ = create_stg_from_csv(src, stg)\n",
    "\n",
    "    # ensure target has audit cols\n",
    "    ensure_target_from_stg(\n",
    "        stg, tgt,\n",
    "        f\"Bronze table for {fname} ingested via COPY INTO; includes audit cols.\"\n",
    "    )\n",
    "\n",
    "    # load into staging\n",
    "    spark.sql(f\"TRUNCATE TABLE {stg}\")\n",
    "    spark.sql(f\"\"\"\n",
    "    COPY INTO {stg}\n",
    "    FROM '{src}'\n",
    "    FILEFORMAT = CSV\n",
    "    FORMAT_OPTIONS ('header'='true', 'inferSchema'='true')\n",
    "    \"\"\")\n",
    "\n",
    "    # insert into target with audit cols\n",
    "    spark.sql(f\"\"\"\n",
    "    INSERT INTO {tgt}\n",
    "    SELECT\n",
    "      s.*,\n",
    "      current_timestamp() AS load_dt,\n",
    "      '{src}' AS source_path,\n",
    "      'copy_into_csv_as_is' AS ingest_mode\n",
    "    FROM {stg} s\n",
    "    \"\"\")\n",
    "\n",
    "    rows = spark.sql(f\"SELECT COUNT(*) AS c FROM {stg}\").collect()[0][\"c\"]\n",
    "    print(\"Rows inserted:\", rows)\n",
    "\n",
    "    # cleanup stage table (optional)\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {stg}\")\n",
    "\n",
    "# Quick evidence\n",
    "display(spark.sql(f\"\"\"\n",
    "SHOW TABLES IN {CAT}.{BRONZE}\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66f4a320-445d-4d25-88e4-247974a3fdd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2) JSON metadata -> Bronze tables (one-time)\n",
    "# all_available_metrics.json is a JSON array of strings -> parse from text\n",
    "raw = spark.read.text(ALL_METRICS_JSON)\n",
    "json_str = raw.select(F.concat_ws(\"\", F.collect_list(\"value\")).alias(\"json\")).first()[\"json\"]\n",
    "\n",
    "df_metrics = (spark.createDataFrame([(json_str,)], [\"json\"])\n",
    "              .select(F.from_json(\"json\", \"array<string>\").alias(\"metrics\"))\n",
    "              .select(F.explode(\"metrics\").alias(\"metric\"))\n",
    "              .withColumn(\"load_dt\", F.current_timestamp())\n",
    "              .withColumn(\"source_path\", F.lit(ALL_METRICS_JSON))\n",
    "              .withColumn(\"ingest_mode\", F.lit(\"json_text_parse\"))\n",
    ")\n",
    "\n",
    "metrics_tgt = f\"{CAT}.{BRONZE}.all_available_metrics_bronze\"\n",
    "df_metrics.write.format(\"delta\").mode(\"overwrite\").saveAsTable(metrics_tgt)\n",
    "spark.sql(f\"COMMENT ON TABLE {metrics_tgt} IS 'List of available metric names from all_available_metrics.json (array of strings).'\")\n",
    "\n",
    "print(\"Loaded metrics:\", df_metrics.count())\n",
    "display(spark.sql(f\"SELECT * FROM {metrics_tgt} LIMIT 20\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0193af7a-1669-412f-9992-0ed2ffead1ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# fields_per_level.json is typically a map<string, array<string>> -> flatten\n",
    "raw2 = spark.read.text(FIELDS_PER_LEVEL_JSON)\n",
    "json_str2 = raw2.select(F.concat_ws(\"\", F.collect_list(\"value\")).alias(\"json\")).first()[\"json\"]\n",
    "\n",
    "df_fields_map = (spark.createDataFrame([(json_str2,)], [\"json\"])\n",
    "                 .select(F.from_json(\"json\", \"map<string,array<string>>\").alias(\"m\")))\n",
    "\n",
    "df_fields = (df_fields_map\n",
    "             .select(F.explode(\"m\").alias(\"level\", \"fields\"))\n",
    "             .select(\"level\", F.explode(\"fields\").alias(\"field\"))\n",
    "             .withColumn(\"load_dt\", F.current_timestamp())\n",
    "             .withColumn(\"source_path\", F.lit(FIELDS_PER_LEVEL_JSON))\n",
    "             .withColumn(\"ingest_mode\", F.lit(\"json_text_parse\"))\n",
    ")\n",
    "\n",
    "fields_tgt = f\"{CAT}.{BRONZE}.fields_per_level_bronze\"\n",
    "df_fields.write.format(\"delta\").mode(\"overwrite\").saveAsTable(fields_tgt)\n",
    "spark.sql(f\"COMMENT ON TABLE {fields_tgt} IS 'Fields per level from fields_per_level.json flattened to (level, field).'\")\n",
    "\n",
    "print(\"Loaded fields rows:\", df_fields.count())\n",
    "display(spark.sql(f\"SELECT * FROM {fields_tgt} ORDER BY level, field LIMIT 50\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_bronze_ingest_supporting_files",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
